{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a8905b7",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "A **Gaussian** curve takes the form $f(x) = \\frac1{\\sigma\\sqrt{2\\pi}} \\exp{\\left[-\\frac12 \\left( \\frac{x-\\mu}{\\sigma} \\right)^2\\right]}$, **naive** comes from the assumptions made, and **Bayes** from Bayesian statistics.\n",
    "\n",
    "## Assumptions of a Naive Bayes\n",
    "- Features are **conditionally independent** given the class; $$P(X|y=k) = \\prod_i P(x_i|y=k)$$\n",
    "- Continuous features follow a Gaussian distribution; each $$P(x_i|y=k)=\\frac1{\\sigma_{k,i}\\sqrt{2\\pi}} \\exp{\\left[-\\frac12 \\left( \\frac{x_i-\\mu_{k,i}}{\\sigma_{k,i}} \\right)^2\\right]}$$\n",
    "\n",
    "---\n",
    "## Bayes' Theorem\n",
    "\n",
    "Given some class $y$, $P(y=k|X) = \\frac{P(y=k)\\cdot P(X|y=k)}{P(X)}$. We are looking at computing two probabilities (one for each label) and siding with the larger one. This means that we don't care about the denominator here, in both cases we are dividing by the same $P(X)$. Here, we mention the first assumption of a Naive Bayes, which allows us to rewrite the formula:\n",
    "\n",
    "$$\n",
    "P(y=k|X) \\propto P(y=k)\\cdot \\prod_{j=1}^d P(x_j|y=k)\n",
    "$$\n",
    "\n",
    "And we model each $P(x_j|y=k)$ over a Gaussian/normal distribution:\n",
    "\n",
    "$$\n",
    "P(x_j|y=k) \\sim \\mathcal{N}(x_j; \\mu_{k, j}, \\sigma_{k, j})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mu_{k, j}=\\frac1{n_k}\\cdot \\sum_{i:y=k} x_{i, j}$, the mean of feature $j$ within all instances of class $k$.\n",
    "- $\\sigma_{k, j}^2 = \\frac1{n_k}\\cdot \\sum_{i:y=k} [x_{i, j}-\\mu_{k, j}]$\n",
    "- $\\mathcal{N}(x_j; \\mu_{k, j}, \\sigma_{k, j})$ is the Gaussian distribution mentioned before.\n",
    "- $P(y=k) = \\frac{n_k}n$\n",
    "\n",
    "---\n",
    "## Underflow\n",
    "\n",
    "Multiplying lots of small probabilities does not work computationally. We don't want to run into this, the probabilities will become too small and floating point storage will mess with our algorithm, so we need to log the probability function so that multiplication becomes addition.\n",
    "\n",
    "$$\n",
    "\\longrightarrow  \\log {P(y=k | X)} \\propto \\log P(y=k) + \\sum_{j=1}^d \\log P(x_j | y=k)\n",
    "$$\n",
    "\n",
    "We are trying to find the class $k$ to maximise the log probabilities, so we are only interested in the final term.\n",
    "\n",
    "$$\n",
    "\\Longrightarrow log \\left(\\mathcal{N}(x_j; \\mu_{k, j}, \\sigma_{k, j})\\right) = \\log \\left(\\frac1{\\sigma_{k,i}\\sqrt{2\\pi}} \\exp{\\left[-\\frac12 \\left( \\frac{x_i-\\mu_{k,i}}{\\sigma_{k,i}} \\right)^2\\right]}\\right) = -\\frac12 \\log(2\\pi\\sigma_{k,j}) - \\frac{(x_j - \\mu_{k,j})^2}{2\\sigma_{k,j}^2} + prior\n",
    "$$\n",
    "\n",
    "Since we only care about the class which has the highest probability, we define our prediction rule as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_k \\left[ \\log P(y = k) + \\sum_{j=1}^{d} \\log \\mathcal{N}(x_j; \\mu_{k,j}, \\sigma_{k,j}) \\right]\n",
    "$$\n",
    "\n",
    "This selects the class $k$ with the highest total log-probability for $X$.\n",
    "\n",
    "---\n",
    "## Recap\n",
    "\n",
    "1. We have continuous data, which we want to figure out the correct class $k$ to label such data as\n",
    "2. We will fit the Naive Bayes by finding the mean and variance for each value, we calculate the prior probability for each class. \n",
    "3. The, we use the normal distribution to estimate the probabilities using the more stable logarithmic method derived above.\n",
    "\n",
    "---\n",
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01c8e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build with numpy\n",
    "import numpy as np\n",
    "\n",
    "# implement as a class (OOP)\n",
    "class GaussianNB:\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X, y = np.asarray(X), np.asarray(y) # ensure we have numpy arrays\n",
    "        self.classes_ = np.unique(y) # what classes we have\n",
    "\n",
    "        # useful constants for the input data, so we can get means/vars/probs\n",
    "        n_classes, n_features = len(self.classes_), X.shape[1]\n",
    "\n",
    "        # initialise variables outside of loop\n",
    "        self.means_ = np.zeros((n_classes, n_features))\n",
    "        self.variances_ = np.zeros((n_classes, n_features))\n",
    "        self.priors_ = np.zeros(n_classes)\n",
    "\n",
    "        for idx, k in enumerate(self.classes_):\n",
    "            X_k = X[y == k] # our class array\n",
    "\n",
    "            # finding params\n",
    "            self.means_[idx] = X_k.mean(axis=0)\n",
    "            self.variances_[idx] = X_k.var(axis=0)\n",
    "            self.priors_[idx] = X_k.shape[0] / X.shape[0]\n",
    "\n",
    "        # model is fitted, can return the instance\n",
    "        return self\n",
    "    \n",
    "    def _log_gaussian(self, X):\n",
    "        num = -0.5 * (X[:, None, :] - self.means_)**2 / self.variances_\n",
    "        log_prob = num - 0.5 * np.log(2 * np.pi * self.variances_, axis=1)\n",
    "        return log_prob.sum(axis=2)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        log_L = self._log_gaussian(X)\n",
    "        log_prior = np.log(self.priors_)\n",
    "\n",
    "        return self.classes_[np.argmax(log_L + log_prior, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2d943",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset\n",
    "\n",
    "Import the wine dataset from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2797b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wine data\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# load some preprocessing and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# random state seed for this build...\n",
    "# ...\n",
    "# drumroll\n",
    "# ...\n",
    "# The Hardy-Ramanujan number\n",
    "R = 1729\n",
    "# The smallest number which can be expressed as the sum of two different cubes in two different ways\n",
    "# See \"mystery.txt\" for the story\n",
    "\n",
    "# load + preprocess\n",
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=R)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
